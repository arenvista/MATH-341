---
id: L02
aliases: []
tags: []
---

# Matrix Multiplying Vectors

> [!def] Matrix Vector Multiplication
>
> $$
> 	Ax =
> 	\begin{bmatrix}
> 		2 & 5 \\
> 		3 & 7
> \end{bmatrix}
> 	\begin{bmatrix}
> 		v_1 \\
> 		v_2
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		2v_1 & + & 5v_2 \\
> 		3v_1 & + & 7v_2
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		7 \\
> 		10
> \end{bmatrix}
> $$
>
> By Vector Linear Combinations
>
> $$
> 	Ax =
> 	\begin{bmatrix}
> 		2 & 5 \\
> 		3 & 7
> \end{bmatrix}
> 	\begin{bmatrix}
> 		v_1 \\
> 		v_2
> \end{bmatrix} =
> 	v_1 \begin{bmatrix} 5 \\ 7 \end{bmatrix} +
> 	v_2 \begin{bmatrix} 5 \\ 7 \end{bmatrix} =
> 	\begin{bmatrix} 7 \\ 10 \end{bmatrix} \\
> $$

# Matrix Matrix Multiplication

> [!def] Matrix Matrix Multiplication
> Consider a matrix:
>
> $$
> 	A = [a_{i,k}]
> $$
>
> Consider a matrix product $C$:
>
> $$
> 	C = AB = [c_{i,j}] = ~?
> $$
>
> Example:
>
> $$
> 	\begin{bmatrix}
> 		a_{1,1} & a_{2,1} \\
> 		a_{1,2} & a_{2,2}
> \end{bmatrix}
> 	\begin{bmatrix}
> 		b_{1,1} & b_{2,1} \\
> 		b_{1,2} & b_{2,2}
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		x_1 & x_2 \\
> 		x_3 & x_4
> \end{bmatrix}
> $$
>
> Observe $x_1$:
>
> $$
> 	x_1 = [a_{1,1}, a_{2,2}]
> 	\begin{bmatrix}
> 		b_{1,1} \\
> 		b_{1,2}
> \end{bmatrix} = a_{1,1}b_{1,1} + a_{2,1}b_{1,2}
> $$
>
> Observe $x_2$:
>
> $$
>     x_2 = [a_{,1}, a_{2,1}] \begin{bmatrix}
>         b_{2,1} , b_{2,2}
> \end{bmatrix}
> $$
>
> This pattern continues.

> [!pf] Inner Product:
> Example (Matrix x Matrix -> Matrix):
>
> $$
> 	\begin{bmatrix}
> 		1 & 2 \\
> 		3 & 4
> \end{bmatrix}
> 	\begin{bmatrix}
> 		5 & 7 \\
> 		6 & 8
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		(1 * 5) + (2 * 6) & (1*7 + 2*8)   \\
> 		(3*5 + 4*6)       & (3*7 + 4 * 8)
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		17 & 23 \\
> 		39 & 53
> \end{bmatrix}
> $$

> [!pf] Outer Product
> Example (Vector x Vector -> Matrix):
>
> $$
> 	\begin{bmatrix}
> 		1 \\
> 		2
> \end{bmatrix}
> 	\begin{bmatrix}
> 		5 & 7
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		5  & 7  \\
> 		15 & 21
> \end{bmatrix} = C_1
> $$
>
> $$
> 	\begin{bmatrix}
> 		2 \\
> 		4
> \end{bmatrix}
> 	\begin{bmatrix}
> 		6 & 8
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		12 & 16 \\
> 		24 & 32
> \end{bmatrix} = C_2
> $$
>
> Example:
>
> $$
> 	\begin{bmatrix}
> 		5  & 7  \\
> 		15 & 21
> \end{bmatrix} +
> 	\begin{bmatrix}
> 		12 & 16 \\
> 		24 & 32
> \end{bmatrix} =
> 	\begin{bmatrix}
> 		17 & 23 \\
> 		39 & 53
> \end{bmatrix} = C_1 + C_2 = C
> $$
>
> > [!cor]
> > Note both $C_1,C_2$ are rank one matricies. Which can be seen clearly as they are computed as linear combinations of a single vector.

# Matrix Factorization Methods
All factorizations methods here are useful for solving ***linear systems***. 
- In other words they can be used to find $x$ in the equation: $Ax = b$
## Key Factorizations
$$
\begin{align}
& A=CR \\
& A=LU \\
& A=QR \\
& S=Q\Lambda Q^T \\
& A = U \Sigma V^T
\end{align}
$$
## CR Factorization

> [!def] CR Factorization
> 
> **Goal:** Given matrix $A$ we want to factor $A$ as:
> 
> $$
> 	A = CR
> $$
> 
> Process:
> 
> 1. $C$ contains the first row independent-columns of $A$
> 
> - Attaining rank($A$) = r
> 
> 2. Constant $R$ so that columns $j$ of $A=CR$ is a combinations of columns of $C$
> 
> > [!pf] Example:
> > 
> > $$
> > 	A =
> > 	\begin{bmatrix}
> > 		1 & 2 & 4 \\
> > 		1 & 3 & 5
> > 	\end{bmatrix} =
> > 	\begin{bmatrix}
> > 		\vec{a_1} & \vec{a_2} & \vec{a_3}
> > 	\end{bmatrix}
> > $$
> > 
> > Observe the linear dependence of $\vec{a_3}$:
> > 
> > $$
> > 	\vec{a_3} = 2 \vec{a_1} + \vec{a_2} \\
> > $$
> > 
> > Observe there is no possible linear combination that yields $\vec{a_1}, \vec{a_2}$.
> > 
> > - Thus these vectors are said to be our linearly independent vectors/columns.
> > 
> > Thus as $C$ is composed of our linearly indepdent columns.
> > 
> > We can construct $A = CR$ as follows:
> > $$
> > 	\begin{array}
> > 		 & A =
> > 		\begin{bmatrix}
> > 			1 & 2 & 4 \\
> > 			1 & 3 & 5
> > 		\end{bmatrix} =
> > 		 & \begin{bmatrix}
> > 			   1 & 2 \\
> > 			   1 & 3
> > 		   \end{bmatrix}
> > 		 & \begin{bmatrix}
> > 			   1 & 0 & 2 \\
> > 			   0 & 1 & 1
> > 		   \end{bmatrix}      \\
> > 		 & C               & R
> > 	\end{array}
> > $$
> > 
> > [!pf] Example (2): 
> > 
> > > [!cor] Linking of Rank and Products
> > > 1. $\forall$ rank one matrix can be writter as an outter product of two, any only two, vectors.
> > > 2. $\forall$ rank $r$ matricies can be written as the sum of $r$ rank one matricies.
> > 
> > $$
> >     A = 
> > \begin{bmatrix}
> >     2 & 4 & 6 \\
> >     3 & 6 & 9 
> > \end{bmatrix} = 
> > \begin{bmatrix} 2 \\ 3 \end{bmatrix}
> > \begin{bmatrix}
> >     1 & 2 & 3 
> > \end{bmatrix} + 
> > $$ 


## LU Factorization


> [!def] LU Factorization
> Consider a square matrix $A$
> 
> Solve $Ax=b$ by the following:
> $$
> \begin{align}
> Ax & = b \\
> & \equiv \\
> L(Ux) & = b
> \end{align}
> $$ 
> 
> Process:
> 1. Find $c$ from $Lc=b$
> 2. Find $x$ from $Ux=c$
> 
> > [!cor] Note: Cost of Computability 
> > - Computing $LU$ is $O(n^3)$ where $n$ is the size (num col/rows) of $A$
> > - Backsubsitution is $O(n^2)$
> > - $LU$ Factorization is good, when the problem has many RHS, and the same matrix $A$

# Solving Linear Systems

## Invertible Matricies

> [!def] Invertible Matrix Theorem:
> - Non-Zero Eigenvalues
> - The only solution to the nullspace is only trivial
> - Rank = Num Col = Num Rows

> [!def] Invertible Matricies
> Consider $Ax=b$
> - $A$ is a square matrix and is invertible
> Consider: $AB = BA = I$
> Formally:
> 1. Start with $Ax=b$
> 2. Multiply both sides by: $A^{-1}$ 
> 3. Thus we can formally define as (below):
> $$
> \begin{align}
> & A^{-1}Ax = A^{-1}b \\ 
> & x = A^{-1}b
> \end{align}
> $$ 
